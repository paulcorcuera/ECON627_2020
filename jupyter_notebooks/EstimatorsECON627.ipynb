{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Review (ECON627 UBC)\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose to store our packages (and their versions), we want to activate our Project.toml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\steve\\Documents\\GitHub\\ECON627_UBC.jl`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(joinpath(pwd(),\"..\")) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "In this course you learned that there is a large class of estimators that, under suitable regularity conditions, admit an asymptotic linear representation: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sqrt n \\left( \\hat{\\theta}_n - \\theta_0 \\right) &= - \\left[ \\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} \\right]^{-1} \\sqrt n \\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} + o_P(1)  \\\\\n",
    "\\sqrt n \\left( \\hat{\\theta}_n - \\theta_0 \\right) &= \\frac{1}{\\sqrt n} \\sum_{i=1}^n \\xi_i + o_P(1) \\quad , \n",
    "\\end{align*}\n",
    "\n",
    "where $\\xi_i$ is known as the influence function, which are i.i.d and centered at zero $\\mathbf{E}[\\xi_i]=0$. By the CLT of i.i.d observations we have that \n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt n \\left( \\hat{\\theta}_n - \\theta_0 \\right) \\rightarrow_d \\mathcal{N}( 0, \\mathbf{E}[\\xi_i \\xi_i'])\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "#### How to know if I'm missing an extra \"n\" when I compute the standard errors?\n",
    "Just follow these steps:\n",
    "\n",
    "- Compute analytically the asymptotic variance.\n",
    "- Replace every expectation by sample average (every sample average must be divided by n!).\n",
    "- The standard errors will be ``sqrt.(diag(avar))/sqrt(n)``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Before we do anything, we set a random seed to ensure replicability of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(1234);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ECON627 package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ECON627_UBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Least Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = \\mathbf{E} \\left[ (Y_i - X_i'\\theta)^2 \\right]/2$\n",
    " - Sample Criterion: $Q_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (Y_i - X_i'\\theta)^2/2$\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} = - \\frac{1}{n} \\sum_{i=1}^n (Y_i-X_i'\\theta_0)X_i $\n",
    " - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = \\frac{1}{n} \\sum_{i=1}^n X_i X_i' \\rightarrow_p \\mathbf{E} X_i X_i'$\n",
    " - Influence function: $\\xi_i :=   (\\mathbf{E}X_i X_i')^{-1} X_i U_i$ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$ (\\mathbf{E}X_i X_i')^{-1} \\mathbf{E}[ U_i^2 X_i X_i'] (\\mathbf{E}X_i X_i')^{-1}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, LinearAlgebra, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_generating_ols (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function data_generating_ols(;n=1000)\n",
    "    #Define the Multivariate Normal Distribution instance as an example\n",
    "    mvnormal = MvNormal([1.0; 2.0], [1 0.7; 0.7 1])\n",
    "    \n",
    "    #Matrix X (n x 2 ) and ε (n x 1)\n",
    "    X = rand(mvnormal,n)\n",
    "    X = transpose(X)\n",
    "    ε = randn(n, 1)\n",
    "    \n",
    "    #Vector y\n",
    "    y = X*[1.0;1.0] + ε\n",
    "        \n",
    "    #Return the data\n",
    "    return (X = X , y = y)\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = [0.9563864710176802; 1.0323477434590973;;], se = [0.0013591890782320168, 0.0008250537779697441])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack X,y = data_generating_ols(n=1000)\n",
    "@unpack θhat, se = ols(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "\n",
    "θcoverage95 = zeros(R)\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack X,y = data_generating_ols(;n=n)\n",
    "    @unpack θhat, se = ols(X,y)\n",
    "    lower_ci  = θhat .- 1.96.*se\n",
    "    upper_ci  = θhat .+ 1.96.*se\n",
    "    \n",
    "    #Coverage of θ[1]\n",
    "    θcoverage95[r] = (lower_ci[1]<1.0)*(upper_ci[1]>1.0)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = \\mathbf{E} \\left[ Z_i (Y_i - X_i'\\theta) \\right]' A'A \\mathbf{E} \\left[ Z_i (Y_i - X_i'\\theta) \\right]/2$ \n",
    " \n",
    " Notice that if there's correct specification, $\\mathbf{E}  \\left[ Z_i (Y_i - X_i'\\theta) \\right] = 0$ ,this population criterion has a minimum value of 0 for any choice of norm $A$.\n",
    " \n",
    " - Sample Criterion: $Q_n(\\theta) = \\frac{1}{n}  \\left[ \\sum_{i=1}^n Z_i (Y_i - X_i'\\theta) \\right]' A_n'A_n  \\left[ \\sum_{i=1}^n Z_i (Y_i - X_i'\\theta) \\right]/2$\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta'} = - \\frac{1}{n} \\sum_{i=1}^n X_i Z_i' A_n' A_n \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i-X_i'\\theta_0)$\n",
    " - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = \\frac{1}{n} \\sum_{i=1}^n X_i Z_i' A_n'A_n  \\frac{1}{n} \\sum_{i=1}^n Z_i X_i' \\rightarrow_p \\Gamma_0 A'A \\Gamma_0$\n",
    " - Influence function: $\\xi_i :=   (\\Gamma_0' A'A \\Gamma_0)^{-1} \\Gamma_0' A'A Z_i U_i$ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$ (\\Gamma_0' A'A \\Gamma_0)^{-1} \\Gamma_0' A'A \\mathbf{E}[ U_i^2 Z_i Z_i'] A'A \\Gamma_0 (\\Gamma_0' A'A \\Gamma_0)^{-1}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, LinearAlgebra, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_generating_gmm (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function data_generating_gmm(;n=1000)\n",
    "   \n",
    "    ϵ=randn(n,1)\n",
    "    V=randn(n,1)\n",
    "    U=0.5*ϵ+V\n",
    "    \n",
    "    Z1=randn(n,1)\n",
    "    Xexo=randn(n,3)\n",
    "    \n",
    "    Xendo=Z1+Xexo*ones(3,1)+V\n",
    "    Y=Xendo+Xexo*ones(3,1)+U\n",
    "\n",
    "    return (Y=Y, Xendo = Xendo, Xexo=Xexo, Z1 = Z1)  \n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_twosls (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function estimate_twosls(Y,X,Z)\n",
    "    #Extract sample size\n",
    "    n = size(Y, 1)\n",
    "    \n",
    "    #Step 1: Estimate theta\n",
    "    Q=Z'*X\n",
    "    W=inv(Z'*Z)\n",
    "    θhat= inv(Q'*W*Q)*Q'*W*(Z'*Y)\n",
    "    \n",
    "    res = Y - X * θhat\n",
    "\n",
    "    #Step 2: Compute Avar (Heteroskedastic Robust) \n",
    "    \n",
    "    zr = Z .* res  \n",
    "    avar = inv(Q'*W*Q/n)* Q'*W*(zr'*zr/n)*W*Q*inv(Q'*W*Q/n) \n",
    "    se = sqrt.(diag(avar))\n",
    "\n",
    "    return (θhat=θhat , se = se )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = [0.9938392308323604; 0.990901289906396; 0.9785738392953602; 0.9930816515690295;;], se = [1.1176338684691591, 1.5939775190723664, 1.5579986598162396, 1.5706964438137758])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack Xendo, Xexo,Y,Z1 = data_generating_gmm(n=1000)\n",
    "X=hcat(Xendo,Xexo)\n",
    "Z=hcat(Xexo,Z1)\n",
    "@unpack θhat, se = estimate_twosls(Y,X,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "\n",
    "θcoverage95 = zeros(R)\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack Xendo, Xexo,Y,Z1 = data_generating_gmm(;n=n)\n",
    "    X=hcat(Xendo,Xexo)\n",
    "    Z=hcat(Xexo,Z1)\n",
    "    @unpack θhat, se = estimate_twosls(Y,X,Z)\n",
    "    \n",
    "    lower_ci  = θhat .- 1.96.*se/sqrt(n)\n",
    "    upper_ci  = θhat .+ 1.96.*se/sqrt(n)\n",
    "    θcoverage95[r] = (lower_ci[1]<1.0)*(upper_ci[1]>1.0)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-linear GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = \\mathbf{E} \\left[g(W_i,\\theta) \\right]' A'A \\mathbf{E} \\left[ g(W_i,\\theta) \\right]/2$ \n",
    " \n",
    " Notice that if there's correct specification, $\\mathbf{E}  \\left[ g(W_i,\\theta)\\right] = 0$ ,this population criterion has a minimum value of 0 for any choice of norm $A$.\n",
    " \n",
    " - Sample Criterion: $Q_n(\\theta) = \\frac{1}{n}  \\left[ \\sum_{i=1}^n g(W_i,\\theta) \\right]' A_n'A_n  \\left[ \\sum_{i=1}^n g(W_i,\\theta) \\right]/2$\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} =  \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial g(W_i,\\theta_0)'}{\\partial \\theta}\\right) A_n'A_n' \\left( \\frac{1}{n} \\sum_{i=1}^n  g(W_i,\\theta_0)\\right)$\n",
    " - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta}\\right) A_n'A_n' \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'}\\right) + \\left[I_k \\otimes \\left( \\frac{1}{n} \\sum_{i=1}^n  g(W_i,\\theta_0)\\right) A_n'A_n\\right] \\left[ \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta'} vec \\left( \\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'} \\right) \\right]$\n",
    " $\\rightarrow_p \\Gamma_0' A'A\\Gamma_0 + \\left[I_k \\otimes \\mathbf{E}[g(W_i,\\theta_0)'A'A] \\right] \\mathbf{E} \\frac{\\partial}{\\partial \\theta'}vec \\left( \\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'} \\right)  $\n",
    " \n",
    " where the second term is 0 if correctly specified. In the influence function below I'll assume it's zero to avoid writing too much algebra.\n",
    " \n",
    " - Influence function: $\\xi_i :=   (\\Gamma_0' A'A \\Gamma_0)^{-1} \\Gamma_0' A'A g(W_i,\\theta_0)$ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$ (\\Gamma_0' A'A \\Gamma_0)^{-1} \\Gamma_0' A'A \\mathbf{E}[ g(W_i,\\theta_0) g(W_i,\\theta_0)'] A'A \\Gamma_0 (\\Gamma_0' A'A \\Gamma_0)^{-1}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, LinearAlgebra, Distributions, Optim, ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_generating_gmm (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function data_generating_gmm(;n=1000)\n",
    "   \n",
    "    ϵ=randn(n,1)\n",
    "    V=randn(n,1)\n",
    "    U=0.5*ϵ+V\n",
    "\n",
    "    #2 instruments for 1 endogenous : overidentified\n",
    "    Z1=randn(n,2)\n",
    "    Xexo=randn(n,3)\n",
    "    \n",
    "    Xendo=Z1*ones(2,1)+Xexo*ones(3,1)+V\n",
    "    Y=Xendo+Xexo*ones(3,1)+U\n",
    "\n",
    "    return (Y=Y, Xendo = Xendo, Xexo=Xexo, Z1 = Z1)  \n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_nlgmm (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function estimate_nlgmm(Y, X, Z,W) \n",
    "    # Obtain sample size\n",
    "    n = size(Y, 1)\n",
    "\n",
    "    #Set up moment condition\n",
    "    g = (y,x,z,θ) -> z*(y-x*θ)\n",
    "    \n",
    "    # Objective function based on moment condition\n",
    "    function sample_moment(Y,X,Z,θ)\n",
    "        v = map(i -> g(Y[i],X[i, :]',Z[i,:]',  θ), 1:n)\n",
    "        v = sum([ v[i] for i in 1:n  ])\n",
    "        v = [v...]\n",
    "        return v\n",
    "    end\n",
    "    obj = θ -> transpose(sample_moment(Y,X,Z,θ)) * W * sample_moment(Y,X,Z,θ)\n",
    "    \n",
    "    #Initial Value \n",
    "    theta0 = zeros(size(X,2))\n",
    "\n",
    "    # We set the criterion function as an instance that we can differentiate twice\n",
    "    td = TwiceDifferentiable(obj, theta0 ; autodiff = :forward)\n",
    "    o = optimize(td, theta0, Newton(), Optim.Options() )\n",
    "\n",
    "    if !Optim.converged(o)\n",
    "        error(\"Minimization failed.\")\n",
    "    end\n",
    "\n",
    "    θhat = Optim.minimizer(o)\n",
    "\n",
    "\n",
    "    # Get asyvar, we compute the gradient of g with respect to theta\n",
    "    # Jacobian will yield dg(W,θ)/dθ' (lxk matrix)\n",
    "    v = map(i -> ForwardDiff.jacobian(θ -> g(Y[i],X[i, :]',Z[i,:]', θ), θhat), 1:n)\n",
    "    dg = sum([ v[i] for i in 1:n ])\n",
    "   \n",
    "    v = map(i -> g(Y[i],X[i, :]',Z[i,:]', θhat), 1:n)\n",
    "    outerprod = sum([ [v[i]...]*[v[i]...]' for i in 1:n ])\n",
    "    \n",
    "    mmd = dg' *W* dg\n",
    "    avar = (mmd/n) \\ dg' *W*(outerprod/n)*W*dg / (mmd/n)\n",
    "\n",
    "    se = sqrt.(diag(avar))\n",
    "\n",
    "\n",
    "    return (θhat=θhat , se = se )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unpack Xendo, Xexo,Y,Z1 = data_generating_gmm(n=1000)\n",
    "X=hcat(Xendo,Xexo)\n",
    "Z=hcat(Xexo,Z1)\n",
    "W = inv(Z'*Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = [1.0220958428305529, 0.9423644898707992, 0.9718141816923822, 0.9672702116728052], se = [0.8008974330527712, 1.458818492880134, 1.3239302309718795, 1.4669834816238212])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack θhat, se = estimate_nlgmm(Y, X, Z,W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "\n",
    "θcoverage95 = zeros(R)\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack Xendo, Xexo,Y,Z1 = data_generating_gmm(;n=n)\n",
    "    X=hcat(Xendo,Xexo)\n",
    "    Z=hcat(Xexo,Z1)\n",
    "    @unpack θhat, se = estimate_nlgmm(Y, X, Z,W) \n",
    "    \n",
    "    lower_ci  = θhat .- 1.96.*se/sqrt(n)\n",
    "    upper_ci  = θhat .+ 1.96.*se/sqrt(n)\n",
    "    θcoverage95[r] = (lower_ci[1]<1.0)*(upper_ci[1]>1.0)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = -\\mathbf{E} \\log f(W_i,\\theta)$ \n",
    " - Sample Criterion: $Q_n(\\theta) = -\\frac{1}{n}   \\sum_{i=1}^n f(W_i,\\theta) $\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} = -\\frac{1}{n}   \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} f(W_i,\\theta_0)$\n",
    " - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = -\\frac{1}{n}   \\sum_{i=1}^n \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} f(W_i,\\theta)$\n",
    " $\\rightarrow_p -\\mathbf{E} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} \\log f(W_i,\\theta_0)  $ \n",
    " - Influence function: $\\xi_i := \\left[ \\mathbf{E} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} \\log f(W_i,\\theta_0)  \\right]^{-1} \\frac{\\partial}{\\partial \\theta} \\log f(W_i,\\theta_0)  $ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$ \\left[ \\mathbf{E} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} \\log f(W_i,\\theta_0)  \\right]^{-1} \\mathbf{E}[ \\frac{\\partial}{\\partial \\theta} \\log f(W_i,\\theta_0) \\frac{\\partial}{\\partial \\theta'} \\log f(W_i,\\theta_0) ] \\left[ \\mathbf{E} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} \\log f(W_i,\\theta_0)  \\right]^{-1} $\n",
    "\n",
    "Under correctly specified MLE the matrix inside $\\Omega_0$ equals the Hessian $B_0$, and the asymptotic variance is reduced to the inverse of the Hessian matrix. If that is not the case, the minimizer is a pseudo-true parameter and this is called the Quasi-MLE approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, LinearAlgebra, Distributions, Optim, ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ϕ (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function data_generating_mle(;n=1000)\n",
    "      \n",
    "    X=randn(n)\n",
    "    u=randn(n)\n",
    "    Y=(u .< 1.0 .+ X)\n",
    "    \n",
    "    return (Y=Y,X=X)\n",
    "end\n",
    "\n",
    "Φ(v)=cdf(Normal(0,1),v)\n",
    "ϕ(v)=pdf(Normal(0,1),v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_MLE (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function LogL(θ,X,Y)\n",
    "    n=length(Y)\n",
    "    LL=0.0\n",
    "    for i=1:n\n",
    "        indx=θ[1]+X[i]*θ[2]\n",
    "        LL -=Y[i]*log(Φ(indx))+(1-Y[i])*log(1-Φ(indx))\n",
    "    end\n",
    "    return LL/n\n",
    "end\n",
    "\n",
    "function estimate_MLE(X,Y)\n",
    "    result=optimize(θ->LogL(θ,X,Y),[0.0;0.0];autodiff = :forward)\n",
    "    θhat = Optim.minimizer(result)\n",
    "    \n",
    "    Var = inv(ForwardDiff.hessian(θ->LogL(θ,X,Y), θhat))\n",
    "        \n",
    "    return (θhat=θhat, se = sqrt.(diag(Var)) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = [0.9983252683833257, 1.0123227876139826], se = [1.8809879670840206, 2.1565735311544567])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack X,Y = data_generating_mle(;n=1000)\n",
    "@unpack θhat, se = estimate_MLE(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "\n",
    "θcoverage95 = zeros(R)\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack X,Y = data_generating_mle(;n=1000)\n",
    "    @unpack θhat, se = estimate_MLE(X,Y)\n",
    "\n",
    "    lower_ci  = θhat .- 1.96.*se/sqrt(n)\n",
    "    upper_ci  = θhat .+ 1.96.*se/sqrt(n)\n",
    "    θcoverage95[r] = (lower_ci[1]<1.0)*(upper_ci[1]>1.0)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Non-linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = \\mathbf{E} \\left[ (Y_i - g(X_i,\\theta))^2 \\right]/2$\n",
    " - Sample Criterion: $Q_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (Y_i - g(X_i,\\theta))^2/2$\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} = - \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) (Y_i-g(X_i,\\theta_0))$\n",
    " - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = \\frac{1}{n} \\sum_{i=1}^n \\{ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)' - U_i \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} g(X_i,\\theta_0) \\}  \\rightarrow_p \\mathbf{E}\\left[ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)'\\right] - \\mathbf{E} \\left[ U_i \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} g(X_i,\\theta_0)\\right]$\n",
    " \n",
    " where the second component is equal to 0 if the model is correctly specified $\\mathbf{E}[U_i \\mid X_i] =0$ and the law of iterated expectations. I will assume the model is correctly specified to simplify the algebra below. \n",
    " \n",
    " - Influence function: $\\xi_i :=   (\\mathbf{E}\\left[ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)'\\right])^{-1} \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) U_i$ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$  (\\mathbf{E}\\left[ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)'\\right])^{-1} \\mathbf{E}[ U_i^2 \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)']  (\\mathbf{E}\\left[ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)'\\right])^{-1}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, LinearAlgebra, Distributions, Optim, ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_generating_nls (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function data_generating_nls(;n=1000)\n",
    "    #Define the Multivariate Normal Distribution instance\n",
    "    mvnormal = MvNormal([1.0; 2.0], [1 0.7; 0.7 1]);\n",
    "    \n",
    "    #Define Logistic Link function \n",
    "    g = (x, b) -> (1 .+ exp.(-x * b)) .^ -1\n",
    "    \n",
    "    #Matrix X (n x 2 ) and ε (n x 1)\n",
    "    X = rand(mvnormal,n)\n",
    "    X = transpose(X)\n",
    "    ε = randn(n, 1)\n",
    "    \n",
    "    #Vector y\n",
    "    Y = g(X,[1.0, -1.0]) + ε\n",
    "        \n",
    "    #Return the data\n",
    "    return (X = X , Y = Y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_nls (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function estimate_nls(f, y, x ) \n",
    "    # Obtain sample size\n",
    "    n = size(x, 1)\n",
    "\n",
    "\n",
    "    # Objective function\n",
    "    obj = b -> sum((y - f(x, b)) .^ 2);\n",
    "    \n",
    "    #Initial Value \n",
    "    beta0 = zeros(size(x,2))\n",
    "\n",
    "    # We set the criterion function as an instance that we can differentiate twice\n",
    "    td = TwiceDifferentiable(obj, beta0 ; autodiff = :forward)\n",
    "    o = optimize(td, beta0, Newton(), Optim.Options() )\n",
    "\n",
    "    if !Optim.converged(o)\n",
    "        error(\"Minimization failed.\")\n",
    "    end\n",
    "\n",
    "    θhat = Optim.minimizer(o)\n",
    "\n",
    "\n",
    "    # Get residuals\n",
    "    r_hat = y - f(x, θhat)\n",
    "\n",
    "    # Get asyvar, we compute the gradient of f with respect to b\n",
    "    v = map(i -> ForwardDiff.gradient(θ -> f(x[i, :]', θ), θhat), 1:n)\n",
    "    md = vcat(v'...)\n",
    "\n",
    "    me = md .* r_hat; mmd = md' * md\n",
    "    avar = (mmd/n) \\ (me' * me/n) / (mmd/n)\n",
    "\n",
    "    se = sqrt.(diag(avar))\n",
    "\n",
    "    return (θhat = θhat, se = se)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = [0.8338776974612384, -0.8611441619742458], se = [7.943177241033485, 5.815116273622467])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack X,Y = data_generating_nls()\n",
    "g = (x, b) -> (1 .+ exp.(-x * b)) .^ -1\n",
    "\n",
    "@unpack θhat, se = estimate_nls(g,Y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "\n",
    "θcoverage95 = zeros(R)\n",
    "g = (x, b) -> (1 .+ exp.(-x * b)) .^ -1\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack X,Y = data_generating_nls(;n=n)\n",
    "    @unpack θhat, se = estimate_nls(g,Y,X)\n",
    "    \n",
    "    lower_ci  = θhat .- 1.96.*se/sqrt(n)\n",
    "    upper_ci  = θhat .+ 1.96.*se/sqrt(n)\n",
    "    θcoverage95[r] = (lower_ci[1]<1.0)*(upper_ci[1]>1.0)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Minimum Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Population Criterion : $Q(\\theta) = (\\pi_0 - g(\\theta))' A'A (\\pi_0 - g(\\theta))/2$, \n",
    " - Sample Criterion: $Q_n(\\theta) = (\\hat{\\pi}_n - g(\\theta))' A_n'A_n (\\hat{\\pi}_n - g(\\theta))/2$\n",
    " \n",
    "  where $\\sqrt n (\\hat{\\pi}_n - \\pi_0 ) \\rightarrow_p \\mathcal{N}(0,V_0)$ is a first-step estimator.\n",
    "\n",
    " - Gradient:  $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta} = - \\left( \\frac{\\partial g(\\theta_0)}{\\partial \\theta} \\right)' A_n'A_n (\\hat{\\pi}_n - g(\\theta_0))$\n",
    "  - Hessian: $\\frac{\\partial Q_n(\\theta_0)}{\\partial \\theta \\partial \\theta'} = \\left( \\frac{\\partial g(\\theta_0)}{\\partial \\theta} \\right)' A_n'A_n  \\left( \\frac{\\partial g(\\theta_0)}{\\partial \\theta} \\right) + \\left[I_k \\otimes (\\hat{\\pi}_n-g(\\theta_0))' A_n'A_n \\right] \\frac{\\partial}{\\partial \\theta'} vec\\left(  \\frac{\\partial g(\\theta_0)}{\\partial \\theta'}\\right) $\n",
    "$\\rightarrow_p \\Gamma_0 A'A\\Gamma_0 + \\left[ I_k \\otimes (\\pi_0-g(\\theta_0))' A'A  \\right] \\frac{\\partial}{\\partial \\theta'} vec\\left(  \\frac{\\partial g(\\theta_0)}{\\partial \\theta'}\\right) $\n",
    " \n",
    " where the second component is equal to 0 if the model is correctly specified $\\pi_0 = g(\\theta_0)$. I will assume the model is correctly specified to simplify the algebra below. \n",
    " \n",
    " - Influence function: $\\xi_i :=   (\\mathbf{E}\\left[ \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0)'\\right])^{-1} \\frac{\\partial}{\\partial \\theta} g(X_i,\\theta_0) U_i$ \n",
    " \n",
    "Therefore, the asymptotic variance is given by \n",
    "$  ( \\Gamma_0 A'A \\Gamma_0 )^{-1} \\Gamma_0 A'A \\xi_i^{\\pi} $, \n",
    "where $\\xi_i^{\\pi}$ is the influence function that comes from $\\sqrt n (\\hat{\\pi}_n - \\pi_0 )$ (i.e. the first step).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quantile Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that quantile regression is a special case where we encounter differentiability issues. The approach taken here is to construct a stochastic equicontinuous process such that we obtain a smooth function where we can apply a mean value expansion around the true value $\\theta_{\\tau,0}$.\n",
    "\n",
    "This approach yields the following asymptotic linear representation\n",
    "\n",
    "\\begin{align*}\n",
    "\\sqrt n (\\hat{\\theta}_{\\tau,n} - \\theta_{\\tau,0} ) &= - \\left( \\mathbf{E}[f(X_i'\\theta_{\\tau,0}) X_i X_i']\\right)^{-1} \\frac{1}{\\sqrt n} \\sum_{i=1}^n \\{  (\\tau - \\mathbf{1}\\{ Y_i < X_i'\\theta_{\\tau,0} \\} X_i \\} + o_P(1)\n",
    "\\end{align*}\n",
    "\n",
    "This implies that the influence function is given by \n",
    "$\\xi_i = \\left( \\mathbf{E}[f(X_i'\\theta_{\\tau,0}) X_i X_i']\\right)^{-1} \\{  (\\tau - \\mathbf{1}\\{ Y_i < X_i'\\theta_{\\tau,0} \\} X_i \\}$, and the asymptotic variance is given by $\\left( \\mathbf{E}[f(X_i'\\theta_{\\tau,0}) X_i X_i']\\right)^{-1}\\mathbf{E}[\\tau (1-\\tau) X_i X_i'] \\left(    \\mathbf{E}[f(X_i'\\theta_{\\tau,0}) X_i X_i']\\right)^{-1}$ by the L.I.E.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, JuMP, HiGHS, Optim, Statistics,Plots, Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "function data_generating_qr(;n=1000)\n",
    "    X=randn(n,1).^2\n",
    "    U=rand(n,1)\n",
    "    Y=X .* (U .-0.5)\n",
    "    return (Y=Y, X=X)\n",
    "end\n",
    "\n",
    "#If you wanna estimate asy-var\n",
    "K(v)= abs(v) - 1 < 0 ?   0.5* (abs(v) - 1) : 0.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_qreg (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function estimate_qreg(;Y,X,τ)\n",
    "    \n",
    "    QR=Model(HiGHS.Optimizer)\n",
    "    set_silent(QR)\n",
    "    n=length(Y)\n",
    "    \n",
    "    c=[τ*ones(n,1);(1-τ)*ones(n,1);];\n",
    "    @variable(QR,x[1:2*n]>=0.0); #this includes the non-negativity constraint on z^{+} and z^{-}\n",
    "    @variable(QR,-100.0<=b<=100.0); #if you want to estimate vector b you use @variable(QR,-100.0<=b[1:k]<=100.0) where k is dimension\n",
    "    @objective(QR,Min,sum(c[i]*x[i] for i in 1:(2*n)));\n",
    "    @constraint(QR,constraint[i in 1:n],x[i]-x[n+i]+sum(X[i,:].*b)==Y[i]); #the equality constraint\n",
    "    optimize!(QR);\n",
    "    estr=value.(b)\n",
    "    \n",
    "    \n",
    "    #Drop this if you don't care about asy-var\n",
    "    \"Calculation of Asymptotic Variance\"\n",
    "    n=length(Y)\n",
    "    rhat= Y - X * estr;\n",
    "    h = 1.06 * std(rhat) * n ^ (-1 / 5);\n",
    "    Khat =  K.(rhat./h) ;\n",
    "\n",
    "    Ωhat = τ * (1 - τ) * (X' * X);\n",
    "    Bhat = ((X.*repeat(Khat,1,size(X,2)))' * X) ./ h;\n",
    "    v = n*(Bhat \\ Ωhat / Bhat);\n",
    "    \n",
    "    \n",
    "    return (θhat = estr , se = sqrt.(diag(v)) )\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(θhat = -0.24898270284504034, se = [0.8658257109940635])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@unpack Y, X =data_generating_qr();\n",
    "@unpack θhat, se=estimate_qreg(;Y=Y,X=X,τ=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=1000\n",
    "R=10^3\n",
    "τcheck=0.75\n",
    "θτ=τcheck-0.5\n",
    "θcoverage95 = zeros(R)\n",
    "\n",
    "Threads.@threads for r=1:R\n",
    "    \n",
    "    @unpack Y, X =data_generating_qr(;n=n);\n",
    "    @unpack θhat, se=estimate_qreg(;Y=Y,X=X,τ=τcheck)\n",
    "    \n",
    "    lower_ci  = θhat .- 1.96.*se/sqrt(n)\n",
    "    upper_ci  = θhat .+ 1.96.*se/sqrt(n)\n",
    "    θcoverage95[r] = (lower_ci[1]<θτ)*(upper_ci[1]>θτ)\n",
    "end\n",
    "\n",
    "sum(θcoverage95)/R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ECON627 package we can use some of these functions that are precompiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b = [1.0663071246108842; 0.9651268464189884;;], se = [0.0013242315493568438, 0.0008359902945974531])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ECON627_UBC\n",
    "# You don't need to hard code estimators anymore :) \n",
    "@unpack X,y = data_generating_ols(n=1000)\n",
    "@unpack b, se = ols(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
