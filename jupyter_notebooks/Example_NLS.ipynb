{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `C:\\Users\\paulc\\.julia\\dev\\ECON627_2020\\Project.toml`\n"
     ]
    }
   ],
   "source": [
    "cd(joinpath(pwd(),\"..\"))\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(\".\") ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Optim and Automatic Differentiation Pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"Optim\")\n",
    "#Pkg.add(\"ForwardDiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Parameters, Optim, ForwardDiff, LinearAlgebra, Distributions, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGP \n",
    "\n",
    "Suppose we have the following non-linear model generated by the logistic link function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "β = [1.0; -1.0];\n",
    "m_fn = (x, b) -> (1 .+ exp.(-x * b)) .^ -1; # Logistic link function\n",
    "\n",
    "x = 2 * rand( n, length(β))\n",
    "u = rand(n,1)\n",
    "y = m_fn(x,β) + u ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the sample moment condition here is that the errors are orthogonal to the gradient of the function evaluated at $\\hat{\\beta}$. In other words,\n",
    "\n",
    "\n",
    "$$ 0 = \\frac{1}{n} \\sum_{i=1}^n (Y_i - f(X_i, \\hat{\\beta}) \\cdot \\frac{\\partial f(X_i, \\hat{\\beta})}{\\partial\\beta}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can compute the gradient at observation 1 and evaluated at $\\hat{\\beta} = [3,-2]$ using the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.2900647377513582\n",
       " 0.4683554496565841"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForwardDiff.gradient(z -> m_fn(x[1, :]', z), [3.0 ; -2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute it for all observations using the `map()` function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Array{Float64,1},1}:\n",
       " [0.2900647377513582, 0.4683554496565841]\n",
       " [0.10053398134796412, 0.06628174136046258]\n",
       " [0.19399676562712462, 0.18232432397782836]\n",
       " [0.2273859726762808, 0.3677378752612256]\n",
       " [0.0262847551718364, 0.005766875473504174]\n",
       " [0.10197880038334955, 0.07277473863927474]\n",
       " [0.07437804513139447, 0.21547855428304005]\n",
       " [0.1491911672038609, 0.2307506304075694]\n",
       " [0.054308175681279804, 0.18807143725686665]\n",
       " [0.2396274604950775, 0.2678559293567952]\n",
       " [0.28238657497254493, 0.3415641492706213]\n",
       " [0.005364359455578175, 0.03956468758145818]\n",
       " [0.01698647048089965, 0.13611592691318045]\n",
       " ⋮\n",
       " [0.029070583551457176, 0.13456507829961215]\n",
       " [0.011718257290027649, 0.0024993215949869296]\n",
       " [0.08612433651200735, 0.04060249009009753]\n",
       " [0.1832384079134702, 0.17591502948557103]\n",
       " [0.05192609392556134, 0.1884843186991116]\n",
       " [0.2876529819960016, 0.42112085402348143]\n",
       " [0.005975597105910294, 0.06503971406834456]\n",
       " [0.01708144678497192, 0.0036041947956482363]\n",
       " [0.16302776484764445, 0.1417560766401169]\n",
       " [0.18923883149855872, 0.2779395061025797]\n",
       " [0.12296841323001381, 0.09150691684010005]\n",
       " [0.0845512698529558, 0.02753121534371778]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = map(i -> ForwardDiff.gradient(z -> m_fn(x[i, :]', z),[3.0 ; -2.0]), 1:n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is an Array of Arrays, so the next code will convert it to a Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float64,2}:\n",
       " 0.290065    0.468355\n",
       " 0.100534    0.0662817\n",
       " 0.193997    0.182324\n",
       " 0.227386    0.367738\n",
       " 0.0262848   0.00576688\n",
       " 0.101979    0.0727747\n",
       " 0.074378    0.215479\n",
       " 0.149191    0.230751\n",
       " 0.0543082   0.188071\n",
       " 0.239627    0.267856\n",
       " 0.282387    0.341564\n",
       " 0.00536436  0.0395647\n",
       " 0.0169865   0.136116\n",
       " ⋮           \n",
       " 0.0290706   0.134565\n",
       " 0.0117183   0.00249932\n",
       " 0.0861243   0.0406025\n",
       " 0.183238    0.175915\n",
       " 0.0519261   0.188484\n",
       " 0.287653    0.421121\n",
       " 0.0059756   0.0650397\n",
       " 0.0170814   0.00360419\n",
       " 0.163028    0.141756\n",
       " 0.189239    0.27794\n",
       " 0.122968    0.0915069\n",
       " 0.0845513   0.0275312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcat(v'...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can minimize the sample criterion function using the function ``Optim.optimize()``, which can take as an input a differentiable object, an initial value, an algorithm (i.e. Newton, LBFGS, etc.), and we're using the default Optim options (tolerance level, max number of iterations, etc.). You can see more details at this [link](http://julianlsolvers.github.io/Optim.jl/v0.9.3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nls (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nls(f, y, x ) \n",
    "\n",
    "    n = size(x, 1)\n",
    "\n",
    "    if n != length(y)\n",
    "        error(\"Incompatible data.\") \n",
    "    end\n",
    "\n",
    "    # Objective function\n",
    "    obj = b -> sum((y - f(x, b)) .^ 2);\n",
    "    \n",
    "    #Initial Value \n",
    "    beta0 = zeros(size(x,2))\n",
    "\n",
    "    # We set the criterion function as an instance that we can differentiate twice\n",
    "    td = TwiceDifferentiable(obj, beta0 ; autodiff = :forward)\n",
    "    o = optimize(td, beta0, Newton(), Optim.Options() )\n",
    "\n",
    "    if !Optim.converged(o)\n",
    "        error(\"Minimization failed.\")\n",
    "    end\n",
    "\n",
    "    βhat = Optim.minimizer(o)\n",
    "\n",
    "\n",
    "    # Get residuals\n",
    "    r_hat = y - f(x, βhat)\n",
    "\n",
    "    # Get asyvar, we compute the gradient of f with respect to b\n",
    "    v = map(i -> ForwardDiff.gradient(z -> f(x[i, :]', z), βhat), 1:n)\n",
    "    md = vcat(v'...)\n",
    "\n",
    "    me = md .* r_hat; mmd = md' * md\n",
    "    avar = mmd \\ (me' * me) / mmd\n",
    "\n",
    "    se = sqrt.(diag(avar));\n",
    "\n",
    "    return (βhat = βhat, se = se )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(βhat = [8.237881250619758, 0.21694719413437033], se = [1.4995775264787221, 0.17344780809640403])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nls(m_fn,y,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
