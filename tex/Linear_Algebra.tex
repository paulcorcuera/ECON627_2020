\documentclass[11pt]{article}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    \usepackage{hyperref}
    \usepackage[authoryear]{natbib}
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi
    % Bibliography
\usepackage[authoryear]{natbib}
\bibliographystyle{chicago}
%\setcitestyle{authoryear,open={(},close={)}}
\usepackage{bibentry}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.36,.54,.66}
    \definecolor{citecolor}{rgb}{.21,.54,.66}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Linear Algebra Notes}
	\author{PaÃºl Corcuera}    
	\date{January, 2021}    
    
    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
        \maketitle
    
    

    
    This notebook is indented to review linear algebra material that will be useful for ECON627 at VSE.

    \tableofcontents
    \newpage

    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Economists often study settings where units possess two or more group
memberships, some of which can change over time. A prominent example
comes from \cite{abowd1999high} (henceforth AKM) who
proposed a panel model of log wage determination that is additive in
worker and firm fixed effects.

This so-called ``two-way'' fixed effects or ``AKM'' model takes the
form:

\begin{equation}
    y_{gt} =  \alpha_{{g}} + \psi_{j({g},t)} + w_{gt}'\delta +  \varepsilon_{gt}  \qquad({g}=1,\dots,N, \ t=1,\dots,T_{g} \ge 2)
\end{equation}

where the function
\(j(\cdot ,\cdot ):\lbrace 1,\dots ,N\rbrace \times \lbrace 1,\dots ,\max_i T_g \rbrace \to \lbrace 1,\dots ,J\rbrace\)
assigns a worker \(g\) and year \(t\) observation to one of \(J\) firms.
Here \(\alpha_g\) is a person effect, \(\psi_{j(g,t)}\) is a firm
effect, \(w_{gt}\) is a time-varying covariate, and \(\varepsilon_{gt}\)
is a mean-independent time-varying error.

We can rewrite the original AKM model as:

\begin{equation}
y_i =x_i^{\prime } \beta +\varepsilon_i \qquad i=1,...,n
\end{equation}

where \(i\) indexes a particular person-year observation \((g,t)\),
\(x_i\) is a vector that collects all the worker, firm dummies as well
as the time-varying covariates \(w_{gt}\) so that
\(\beta =(\alpha ,\psi ,\delta )'\) is a \(k\times 1\) vector that
collects all the worker, firm fixed effects along with \(\delta\).

Interest in AKM models often centers on understanding how much of the
variability in log wages is attributable to firms. It is common to
summarize the firm contribution to wage inequality using the following
two parameters:

\begin{equation}
\sigma_{\psi }^2 =\frac{1}{n}\sum_{g=1}^N \sum_{t=1}^{T_g } {\left(\psi_{j\left(g,t\right)} -\bar{\psi} \right)}^2 \qquad \text{and }\sigma_{\alpha ,\psi } =\frac{1}{n}\sum_{g=1}^N \sum_{t=1}^{T_g } \left(\psi_{j\left(g,t\right)} -\bar{\psi} \right)\alpha_g           
\end{equation}

where
\(\bar{\psi} =\frac{1}{n}\sum_{g=1}^N \sum_{t=1}^{T_g } \psi_{j(g,t)}\).
The variance component \(\sigma_{\psi }^2\) measures the contribution of
firm wage variability to inequality, while the covariance component
\(\sigma_{\alpha ,\psi }\) measures the additional contribution of
systematic sorting of high wage workers to high wage firms.

The function \texttt{leave\_out\_KSS} provides unbiased estimates of
\(\sigma_{\psi }^2\) and \(\sigma_{\alpha ,\psi }\) as well as an
estimate of
\(\sigma_{\alpha }^2 =\frac{1}{n}\sum_{g=1}^N \sum_{t=1}^{T_g } {\left(\alpha_g -\bar{\alpha} \right)}^2\)
using the leave-out bias correction approach proposed by KSS.

\hypertarget{the-kss-correction}{%
\subsection{The KSS Correction}\label{the-kss-correction}}

We now provide some general intuition about the KSS leave-out
methodology. A more formal discussion can be found in KSS.
\hypertarget{the-plug-in-estimator}{%
\subsubsection{The Plug-in Estimator}\label{the-plug-in-estimator}}

Suppose the researcher is interested in the variance of firm effects,
\(\sigma_{\psi }^2\). To simplify the exposition, we normalize the firm
effects so that their firm-size weighted mean is equal to zero,
i.e.~\(\bar{\psi}=0\) and re-write \(\sigma_{\psi }^2\) as

\begin{equation}
\sigma_{\psi }^2 =\sum_{j=1}^J s_{j}\psi^{2}_{j}
\end{equation}

where $s_{j}$ gives the employment share of firm $j$, $s_{j}=\dfrac{1}{n}\sum_{g=1}^{N}\sum_{t=1}^{T_{g}}\mathbf{1}\{j(g,t)=j\}$.

It is customary to report ``plug-in'' estimates of a given variance
component using the corresponding OLS estimate. For instance, the
plug-in estimate of the variance of firm effects \(\sigma_{\psi }^2\) is
given by

\begin{equation}
\tilde{\sigma}_{\psi}^2=\sum_{j=1}^J s_{j}\hat{\psi}^{2}_{j}
\end{equation}

where \(\hat{\psi}_{j}\) is the OLS estimate obtained after estimating
equation (1) via OLS.

\hypertarget{the-bias-in-the-plug-in-estimator}{%
\subsubsection{The Bias in the Plug-in
Estimator}\label{the-bias-in-the-plug-in-estimator}}

The estimated firm effect, \(\hat{\psi}_{j}\), represents a noisy
estimate of the true firm effect, \(\psi_{j}\). The presence of noise in
\(\hat{\psi}_{j}\) is not an issue when one is interested in
\(\psi_{j}\) as the OLS estimator \(\hat{\psi}_{j}\) is assumed to be
unbiased in this context, i.e.~\(E[\hat{\psi}_{j}]=\psi_{j}\).

However, estimation error in \(\hat{\psi}_{j}\) is going to lead to
biases when estimating \(\psi_{j}^{2}\) using its ``plug-in'' analogue
\(\hat{\psi}_{j}^{2}\) since:

\begin{equation}
E[\hat{\psi}_{j}^{2}]=E[(\hat{\psi}_{j}-\psi_{j}+\psi_{j})^2]=\psi^{2}+\underbrace{\mathbb{V}[\hat{\psi}_{j}]}_{\text{Bias}}
\end{equation}

where \(\mathbb{V}[\hat{\psi}_{j}]\) is the squared standard error of
\(\hat{\psi}_{j}\). Intuitively, when we take the square of
\(\hat{\psi}_{j}\) we are not only squaring its signal, \(\psi_{j}\),
but also the estimation error in each \(\hat{\psi}_{j}\). The latter is
going to introduce a bias when estimating \(\psi^{2}_{j}\).

The same logic applies when analyzing the bias of the plug-in estimator
of the variance of firm effects since

\begin{equation}
E[\tilde{\sigma}_{\psi}^2]=\sigma^{2}_{\psi}+\underbrace{\sum_{j=1}^{J}s_{j}{\mathbb{V}[\hat{\psi}_{j}]}}_{\text{Bias}}
\end{equation}

\hypertarget{the-problem-with-standard-standard-errors-in-high-dimensional-models}{%
\subsubsection{The Problem with ``Standard'' Standard Errors in High
Dimensional
Models}\label{the-problem-with-standard-standard-errors-in-high-dimensional-models}}

The formula above shows that the bias of the plug-in estimator of the
variance of firm effects is

\begin{equation}
E[\tilde{\sigma}_{\psi}^2]-\sigma^{2}_{\psi}=\sum_{j=1}^{J}s_{j}{\mathbb{V}[\hat{\psi}_{j}]}.
\end{equation}

Therefore, all that is required for a bias correction is an estimate of
the (squared) standard error of each firm effect,
\({\mathbb{V}[\hat{\psi}_{j}]}\). Similarly, if our interest is on the
variance of person effects, then we would need the standard error on
each of the person effects, \({\mathbb{V}[\hat{\alpha}_{i}]}\). If we
are interested in the covariance of worker and firm effects, then we
would need the covariances in sampling errors between each
\(\hat{\alpha}_{i}\) and \(\hat{\psi}_{j(i,t)}\).

The discussion above highlights that an estimate of the sampling
variability of the OLS coefficient vector
\(\hat{\beta}=(\hat{\alpha},\hat{\psi},\hat{\delta})\) is required in
order to derive an unbiased estimate of the variance components of the
AKM model displayed in equation (2).

Recall that the sampling variability in \(\hat{\beta}\), assuming
independence across observations, is given by

\begin{equation}
{\mathbb{{V}}}[\hat{\beta}]=\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\sum_{i=1}^{n}{\sigma}^{2}_{i}x_{i}x_{i}'\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}.
\end{equation}

where \(\sigma^{2}_{i}=Var(\varepsilon_{i})\).

One might be tempted to provide an estimate of \(\mathbb{V}[\hat{\beta}]\)
using heteroskedasticity \emph{consistent} (``HC'' or robust), standard
errors. Standard \cite{white1980heteroskedasticity} HC standard-errors are calculated using a plug-in
estimate of \(\sigma^{2}_{i}\) based on

\begin{equation}
\tilde{\sigma}^{2}_{i}=(y_{i}-x_{i}'\hat{\beta})^2.
\end{equation}

so that the HC based estimate of \(\mathbb{V}[\hat{\beta}]\) is given by

\begin{equation}
\tilde{\mathbb{{V}}}[\hat{\beta}]=\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\sum_{i=1}^{n}\tilde{{\sigma}}^{2}_{i}x_{i}x_{i}'\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}.
\end{equation}

However, HC standard errors based on \(\tilde{\sigma}^{2}_{i}\) are downward biased \citep{mackinnon1985some}. From an asymptotic perspective, HC standard errors are
inconsistent in any high dimensional model where the number of
parameters grows in proportion to the sample size \citep{cattaneo2017inference}. Such "many regressor" asymptotics are natural in the worker-firm fixed effects model  as we often have fewer than 5 worker moves on average per firm.

\hypertarget{the-leave-out-correction}{%
\subsubsection{The Leave Out
Correction}\label{the-leave-out-correction}}

KSS provide a heteroskedasticity-\emph{unbiased} (HU) estimate of the
standard error of any coefficient obtained from a
linear regression model.

The KSS HU standard error estimate is based on a leave-out estimate of
\(\sigma^{2}_{i}\):

\begin{equation}
\hat{\sigma}^{2}_{i}=y_{i}(y_{i}-x_{i}'\hat{\beta}_{-i}).
\end{equation}

where \(\hat{\beta}_{-i}\) is the OLS estimate of \(\beta\) from
equation (2) after leaving observation \(i\) out.

KSS then replaces \(\sigma^{2}_{i}\) in \(\mathbb{V}[\hat{\beta}]\) with
its unbiased estimate \(\hat{\sigma}^{2}_{i}\) to derive a HU estimate
of \(\mathbb{V}[\hat{\beta}]:\)

\begin{equation}
\hat{\mathbb{{V}}}[\hat{\beta}]=\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\sum_{i=1}^{n}\hat{{\sigma}}^{2}_{i}x_{i}x_{i}'\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}.
\end{equation}

Going back to the example of the variance of the firm effects, we can
extract from \(\hat{\mathbb{V}}[\hat{\beta}]\) the corresponding squared
standard error of each firm effect, \(\hat{\mathbb{V}}[\hat{\psi}_{j}]\).
We can then use the latter to bias correct the corresponding estimate of
the variance of the firm effects as follows:

\begin{equation}
\hat{\sigma}^{2}_{\psi}=\tilde{\sigma}^{2}_{\psi}-\sum_{j=1}^{J}s_{j}{\mathbb{V}[\hat{\psi}_{j}]}.
\end{equation}

The MATLAB function \texttt{leave\_out\_KSS} is going to print the bias
corrected variance of firm effects, \(\hat{\sigma}^{2}_{\psi}\), as well
as the bias-corrected covariance of worker and firm effects and variance
of person effects.  \texttt{leave\_out\_KSS} also provide the correct standard errors --- based on \(\hat{\mathbb{V}}[\hat{\beta}]\)  as opposed to \(\tilde{\mathbb{V}}[\hat{\beta}]\) --- when regressing the firm effects on some observable characteristics, see Section \ref{sec:lincom}.
\hypertarget{computing-the-kss-correction}{%
\section{Computing the KSS
Correction}\label{computing-the-kss-correction}}

We now demonstrate how one can implement the KSS correction in two-way
models using MATLAB and the function \texttt{leave\_out\_KSS}. We
continue to work with a simple example based on an AKM model. In what
follows, we use the words ``workers'' and ``firms'' when describing the
procedure for simplicity but the the function \texttt{leave\_out\_KSS}
can be applied to any two-way fixed effects model (e.g.~patients and doctors,
students and teachers, strata and treatment arms).

\hypertarget{setup}{%
\subsection{Setup}\label{setup}}

We begin with some auxiliary lines of code that define the relevant
paths, call the \href{http://www.cs.cmu.edu/jkoutis/cmg.html}{CMG package} package developed by Yiannis Koutis and set-up the
parallel environment within MATLAB.


    \hypertarget{importing-the-data}{%
\subsection{Importing the Data}\label{importing-the-data}}

The Github Repo contains a matched employer-employee testing data where
we observe the identity of the worker, the identity of the firm
employing a given worker, the year in which the match is observed
(either 1999 or 2001) and the associated log wage.

\emph{Important!:} the original data must be sorted by individual
identifiers (id). For instance, one can see that the testing data is
sorted by individual identifiers (and year, using
\texttt{xtset\ id\ year} in Stata)


    \hypertarget{calling-the-main-function}{%
\subsection{Calling the Main Function}\label{calling-the-main-function}}

The function \texttt{leave\_out\_KSS} relies on three mandatory inputs:
\texttt{(y,id,firmid)}. We can obtain an unbiased variance decomposition
of the associated AKM model by simply calling

 
    \hypertarget{interpreting-the-output}{%
\section{Interpreting the Output}\label{interpreting-the-output}}

The code starts by printing its two key inputs: the algorithm used to compute the statistical leverages (exact vs. JLA) --- we explain this distinction in Section \ref{sec:JLA} --- and the level at which the leave-out correction is carried (observation vs. match) --- we explain this in more detail in Section \ref{sec:leave_out_level}. 

The output printed by  \texttt{leave\_out\_KSS}  is composed by
three sections.

\textbf{Section 1}: Here we provide info on leave-out connected set.
This is the largest connected set of firms that remains connected after
removing any worker from the associated graph, see Lemma 1 and the
Computational Appendix of KSS for details.
The code provides some summary statistics (e.g.~\# of movers, \# of
firms, mean and variance of the outcome, etc) for the leave-out
connected set.

\textbf{Section 2}: After printing the summary statistics, the code
computes the statistical leverages of the design, denoted as \(P_{ii}\).
Computation of \(\{P_{ii}\}_{i=1}^{n}\) represents the main
computational bottleneck of the routine.

\textbf{Section 3}: The code then enters its third, and final, stage
where the main results are printed. The code starts by reporting the ---
biased --- estimates of the variance components that result from the
``plug-in'' approach of treating OLS estimates as measured without
error. Finally, the code prints the bias corrected variance of firm
effects and the covariance of worker and firm effects.

    \hypertarget{what-does-the-code-save}{%
\section{What Does the Code Save?}\label{what-does-the-code-save}}

\texttt{leave\_out\_KSS} saves three scalars: the variance of firm
effects (\texttt{sigma2\_psi} in {[}4{]}), the covariance of worker and
firm effects (\texttt{sigma\_psi\_alpha}) and the variance of person
effects (\texttt{sigma2\_alpha}).

\texttt{leave\_out\_KSS} also saves on disk one .csv file. This .csv
contains information on the leave-out connected set. This file has 4
columns. First column reports the outcome variable, second and third
columns the worker and the firm identifiers (as originally inputted by
the user). The fourth column reports the statistical leverages of the
regression design. If the code is reporting a leave-out correction at the
match-level, the .csv will be collapsed at the match level. By default,
the .csv file is going to be saved in the main directory under the name
\texttt{leave\_out\_estimates}. The user can specify an alternative path
using the option \texttt{filename} when calling
\texttt{leave\_out\_KSS}.

    \hypertarget{scaling-to-large-datasets}{%
\section{Scaling to Large Datasets}\label{scaling-to-large-datasets}}
\label{sec:JLA}
\texttt{leave\_out\_KSS} can be used on extremely large datasets. The
code uses a variant of the random projection method, denoted as the
Johnson-- Lindenstrauss Approximation (JLA henceforth) algorithm in KSS
for its connection to the work of \cite{johnson1984extensions}, see
also \cite{achlioptas2001database}. We now discuss briefly the main computational
bottleneck of the procedure and the JLA algorithm.

\hypertarget{computational-bottleneck}{%
\subsection{Computational Bottleneck}\label{computational-bottleneck}}

Recall from the discussion in Section 1 that the KSS leave-out bias
correction procedure relies on leave-out estimates of \(\sigma^{2}_{i}\)

\begin{equation}
\hat{\sigma}^{2}_{i}=y_{i}(y_{i}-x_{i}'\hat{\beta}_{-i})
\end{equation}

where \(\hat{\beta}_{-i}\) is the OLS estimate of \(\beta\) from the AKM
model in equation (2) after leaving observation \(i\) out.

Clearly, re-estimating \(\hat{\beta}_{-i}\) by leaving a particular
observation \(i\) for \(n\) times, is infeasible computationally.
Fortunately, one can re-write \(\hat{\sigma}^{2}_{i}\) as

\begin{equation}
\hat{\sigma}^{2}_{i}=y_{i}\frac{(y_{i}-x_{i}'\hat{\beta})}{1-P_{ii}}
\end{equation}

where \(P_{ii}\) measures the influence or leverage of observation
\(i\), i.e.~\(P_{ii} =x_i^{\prime } S_{xx}^{-1} x_i\). The expression
above highlights that all that is needed for computation of
\(\hat{\sigma}^{2}_{i}\) are the \(n\) statistical leverages
\(\{P_{ii}\}_{i=1}^{n}\). However, exact computation of \(P_{ii}\) may remain prohibitive when \(n\) is in order of tens of millions or
higher.

\hypertarget{the-jla-algorithm}{%
\subsection{Approximating the Statistical Leverages}\label{the-jla-algorithm}}

The JLA algorithm introduced by KSS provides a stochastic approximation
to \(\{P_{ii}\}_{i=1}^{n}\) using the random projection ideas developed
by Johnson and Lindenstrauss (1984). We defer the reader to the
\href{https://www.dropbox.com/s/ycvls8pbtxewj06/DataComputationAppendix.pdf?dl=1}{Computational Appendix of KSS} for further details.

The number of simulations underlying the JLA algorithm is governed by
the input \texttt{simulations\_JLA} (which is denoted as \(p\) in the
computational appendix). Intuitively, more simulations imply a
higher accuracy -- but higher computation time --- when estimating
\(\lbrace P_{ii} ,B_{ii} \rbrace_{i=1}^n\).

\textbf{Note:} The user might want to pre-specify a random number
generator seed to ensure replicability when calling the function
\texttt{leave\_out\_KSS}.

We now demonstrate the performance of the code on a large dataset



    We can see from the output that the leave-out connected set has almost
14 million person-year observations. The code is able to complete in
around 4 minutes (on a 2020 Macbook pro with 6 cores and 16GB of RAM).

The computational appendix in KSS shows that the JLA algorithm can cut
computation time by a factor of 100 while introducing an approximation
error of roughly \(10^{-4}\).

The current code uses an improved estimator of both \(P_{ii}\) and
\(M_{ii}=1-P_{ii}\) which are both guaranteed to lie in \([0,1]\). These
improved estimators are then combined to derive an asymptotically unbiased JLA
estimator of a given variance component provided that
\(\frac{n}{p^{4}}=o(1)\), see \href{https://www.dropbox.com/s/i28yvzae2tnp2tl/improved_JLA.pdf?dl=1}{this document}.

One can check the stability of the estimates for different values of
\texttt{simulations\_JLA}. For instance, if we double
\texttt{simulations\_JLA} from 50 to 100 and run the code again on the
same data:



    We obtain virtually the same variance components as when
\texttt{simulations\_JLA}=50 while significantly increasing the
computational time! If the user does not specify a value for
\texttt{simulations\_JLA}, the code defaults to
\texttt{simulations\_JLA}=200.

We conclude this section by noting that the user can also calculate an
exact version of \(\lbrace P_{ii} \rbrace_{i=1}^n\). This can be done by
setting the option \texttt{type\_of\_algorithm} to \texttt{exact}.

\textbf{Warning:} Calling the option \texttt{exact} in large datasets
can be very time consuming! We now load again the original, smaller,
testing data and then compare the exact and JLA based estimates of the
variance components

    The variance components estimated via JLA are extremely close to the
\texttt{exact} estimates but only take a fraction of the time to compute. If
the input data has more than 10,000 obs, the code defaults to using the
JLA algorithm unless the user specifies type\_of\_algorithm to
``exact''.

    \hypertarget{adding-controls}{%
\section{Adding Controls}\label{adding-controls}}

We have demonstrated the functioning of \texttt{leave\_out\_KSS} using a
simple AKM model with no controls (\(w_{gt}=0\)). It is easy to add a
matrix of controls to the routine. Suppose for instance that we want to
add year fixed effects to the original AKM model. This can be done as
follows



    When controls are specified, the code proceeds by partialling them out.
That is, it first estimates by OLS the AKM model in the leave-out connected set

\begin{equation}
y_{gt}=\alpha_{g}+\psi_{j(g,t)}+w_{gt}'\delta+\varepsilon_{gt}
\end{equation}

from which we obtain \(\hat{\delta}\). We then work with a residualized
model where the outcome variable is now defined as
\(y_{gt}^{new}=y_{gt}-w_{gt}'\hat{\delta}\) and project this
residualized outcome on worker and firm indicators and report the
associated (bias-corrected) variance components.

\section{Leaving out a Person-Year Observation vs.~Leaving Out a
Match}
\label{sec:leave_out_level}

By default, the code reports leave-out corrections for the variance of
firm effects and the covariance of firm and worker effects that are
robust to unrestricted heteroskedasticity and serial correlation of the
error term within a given match (unique combination of worker and firm
identifier), see Remark 3 of KSS. Intuitively, leaving out matches is analogous to "clustering" the standard error estimates on match. We discuss the interpretation of the
leave-out corrected variance of person effects when leaving a match out
in Section \ref{sec:var_pe}.

The user can specify the function to run the KSS correction when leaving
only an observation out using the option \texttt{leave\_out\_level}.
When leaving a person-year observation out, the resulting KSS variance
components are robust to unrestricted heteroskedasticity but not serial
correlation within match. Below we demonstrate how to compute KSS
adjusted variance components when leaving a single (person-year)
observation out.


    When \(T=2\) (i.e the underlying matched employer-employee data spans
only two years), as in this example, it turns out that the KSS adjusted
variance of firm effects and covariance of firm and worker effects is
robust to any arbitrary correlation between \(\varepsilon_{g2}\) and
\(\varepsilon_{g1}\).

    \hypertarget{variance-of-person-effects-when-leaving-out-a-match}{%
\section{Variance of Person Effects when Leaving Out a
Match}\label{sec:var_pe}}

By leaving a match-out, we can bias correct the variance of firm
effects and covariance of worker and firm effects while allowing for
unrestricted hetoreskedasticity and serial correlation of the error term
\(\varepsilon_{gt}\) within each worker-firm match.

However, the person effects, \(\alpha_{g}\), of ``stayers'' --- workers
that never leave a particular firm --- are not leave-match-out
estimable. This implies that we cannot compute an unbiased estimate of
\(\Omega_{g}=Var(\varepsilon_{g1},...,\varepsilon_{gT_{g}})\) for
stayers. An estimate of \(\Omega_{g}\) for both stayers and movers is
required in order to provide a bias correction for the variance of
person effects, see Section 1 and Remark 3 in KSS.

The current implementation of the code estimates 
$\Omega_{g}$ for stayers by leaving only a single observation out -- i.e., by assuming $\Omega_g$ is diagonal. This approach yields an upper bound estimate on
the variance of person effects (computed across both stayers and
movers).

There are several alternatives that the user can explore:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate a variance decomposition in a sample of movers only: For
  movers, it is possible to estimate a leave-out bias corrected variance
  of person effects that is robust to both unrestricted
  heteroskedasticity and serial correlation in the error term of the AKM
  model within a given match. Therefore, one can provide an unbiased
  variance decomposition of all the three components of the two-way
  fixed effects model by simply feeding to the function
  \texttt{leave\_out\_KSS} a movers-only sample.
\item
  Drop adjacent wage observations for stayers: Under the assumption that
  the errors are serially independent after m periods, it suffices to
  keep every \(mth\) stayer observation and apply the leave person-year
  out estimator. For example, if \(m=2\) and we have a balanced panel
  with \(T=5\), we can restore independence of the errors in the stayer
  sample by keeping any of the following pairs of stayer time periods:
  (1,4), (2,5), (1,5). One can choose from the available pairs randomly
  for each stayer with equal probability.
 \item 
  Drop interior wage observations for stayers:  To minimize concerns regarding serial correlation, the user can drop all but the first and last wage observation of each stayer. Note that dropping stayer wage observations reduces their weight in the variance components. Future versions of the code will allow the variance components to be defined in terms of weights other than the number of micro-observations.
\end{enumerate}

    \hypertarget{regressing-firm-fixed-effects-on-observables}{%
\section{Regressing Firm Fixed Effects on
Observables}\label{regressing-firm-fixed-effects-on-observables}}
\label{sec:lincom}
It is common in empirical applications to regress the fixed effects
estimated from the two-way model on some observables characteristics.
Using the AKM model again as our leading example, suppose we are
interested in the linear projection of the firm effects \(\psi_{gt}\)
on some observables \(Z_{gt}\):

\begin{equation}
\psi_{j(g,t)}=Z_{gt}'\gamma+e_{gt}
\end{equation}

Typical practice is to estimate \(\gamma\) using a simple regression
where the estimated firm effects, \(\hat{\psi}_{j(g,t)}\), are regressed
on \(Z_{gt}\)

\begin{equation}
\hat{\gamma}=\left(\sum_{g,t}Z_{gt}Z_{gt}'.
\right)^{-1}Z_{gt}\hat{\psi}_{gt}
\end{equation}

KSS show that inference on \(\hat{\gamma}\) needs to be adjusted because the estimated firm fixed effects $\{\hat{\psi}_{j}\}_{j=1}^{J}$ are correlated with one another.

To see this, suppose that we have a simple AKM model with only two time
periods, set \(w_{gt}=0\), and take first differences
\(\Delta y_{g}\equiv y_{g2}-y_{g1}\) to eliminate the worker fixed
effects so that the AKM model becomes \begin{equation}
\Delta y_{g}=\Delta f_{g}'\psi+\varepsilon_{g}
\end{equation}

where \(\Delta f_{g}=f_{g,2}-f_{g,1}\) and
\(f_{gt}=\{\mathbf{1}_{j(g,t)=1},..,\mathbf{1}_{j(g,t)=J}\}\) is the
vector containing the firm dummies. In this model,

\begin{equation}
\hat{\psi}=\psi+\underbrace{\sum_{g=1}^{N}(\Delta f_{g}\Delta f_{g}')^{-1}\Delta f_{g}\varepsilon_{g}}_{\text{Correlated Noise}}
\end{equation}

Note how the dependence in the vector of estimated firm fixed effects,
\(\hat{\psi}\), is induced by the regressor design
\(\sum_{g=1}^{N}(\Delta f_{g}\Delta f_{g}')^{-1}\). As shown in
Table 3 of KSS, ignoring this correlation can easily lead standard
errors to be underestimated by an order of magnitude in practice.

The package provides the HU standard errors on \(\hat{\gamma}\)
using the function \texttt{lincom\_KSS} which is designed to emulate the Stata
function \href{https://www.stata.com/manuals13/rlincom.pdf}{lincom} and therefore works as a post-estimation
command. We demonstrate the functioning of \texttt{lincom\_KSS} with an
example.

In this example, we are interested in testing whether the difference in
person-year weighted mean firm effects between region 1 and region 2 is
statistically different from zero. This amounts to running a regression
where the dependent variable is the vector of estimated firm effects and
the set of observables, \(Z_{gt}\) , here is represented by a constant
and a dummy for whether the firm of worker \(g\) in year \(t\) belongs
to region 2.

The resulting coefficient (and standard error) can be computed by
calling the function \texttt{leave\_out\_KSS} specifying that we want to
run the \texttt{lincom} option and using the region dummy as \(Z_{gt}\)
(the constant is automatically added by the code).



    We can see from the output above (make sure to scroll until the end)
that the difference in person-year weighted mean firm effects between
the two regions is equal to 0.26. The traditional
HC or ``robust'' standard errors on this
coefficient is around 0.05 while the HU standard error derived in KSS is
roughly twice as large (0.09).


    \bibliography{lit}
    

\end{document}